# comments
https://github.com/cakesolutions/scala-kafka-client/issues/147
https://github.com/confluentinc/schema-registry/issues/789
# TODO add answer
https://stackoverflow.com/questions/39670691/how-to-have-kafkaproducer-to-use-a-mock-schema-registry-for-testing

# TODO
https://github.com/ovotech/kafka-serialization
http://vectos.net/formulation

* multi-version on same topic
* tests: https://github.com/confluentinc/schema-registry/blob/master/client/src/main/java/io/confluent/kafka/schemaregistry/client/MockSchemaRegistryClient.java

# TODO

Schema ID allocation always happen in the master node and they ensure that the Schema IDs are monotonically increasing

Kafka is used as Schema Registry storage backend.
The special Kafka topic <kafkastore.topic> (default _schemas), with a single partition, is used as a highly available write ahead log.
All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log.

Schema Registry is designed to work as a distributed service using single master architecture
Only the master is capable of publishing writes to the underlying Kafka log, but all nodes are capable of directly serving read requests.
Slave nodes serve registration requests indirectly by simply forwarding them to the current master, and returning the response supplied by the master.

---

https://github.com/cakesolutions/scala-kafka-client
https://github.com/lightbend/kafka-streams-scala
https://github.com/lightbend/kafka-with-akka-streams-kafka-streams-tutorial
https://github.com/Spinoco/fs2-kafka

---

# fix test java.lang.OutOfMemoryError: Metaspace
https://medium.com/@jan______/sbtconfig-is-deprecated-650d6ff10236

---

https://fd4s.github.io/fs2-kafka

---

version: "3.7"

# https://github.com/confluentinc/cp-docker-images/blob/5.3.2-post/debian/kafka-connect-base/Dockerfile#L59
    healthcheck:
      test: ["CMD", "/etc/confluent/docker/healthcheck.sh"]
      interval: 10s
      timeout: 10s
      retries: 100
      start_period: 2m

---

"key.converter.schemas.enable":"false",
"value.converter.schemas.enable":"true",
"value.converter":"org.apache.kafka.connect.json.JsonConverter"

jdbc-sink-json-schema
https://stackoverflow.com/questions/45928768/kafka-connect-jdbc-sink-connector-not-working/45940013#45940013
https://github.com/apache/kafka/blob/2.4.0/connect/json/src/main/java/org/apache/kafka/connect/json/JsonSchema.java#L39-L59
